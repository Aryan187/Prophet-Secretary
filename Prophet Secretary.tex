\documentclass[10pt, letterpaper, twoside]{article}
\usepackage[left=2cm, right=2cm, top=1cm]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{xcolor}
\usepackage{hyperref}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
%opening
\title{Prophet Secretary: Simplified Analyses\\ of Correa-Sauna-Zilliotto Algorithms}
\author{Aryan Agarwal \and Ashish Chiplunkar \and Siddhant Choudhary}
\date{}

\begin{document}

\maketitle

\section{Introduction}

We present here simplified analyses of the algorithms by Correa, Sauna, and Zilliotto~\cite{CorreaSZ19} for the prophet secretary problem. We consider the real-time version of the problem where the realization of every random variable is seen at an independent random instant drawn from the uniform distribution on $[0,1]$ (and therefore, the realizations are seen in a uniformly random order, as required by the definition of the prophet secretary problem).

Let $X_1,\ldots,X_n$ be independent non-negative random variables with CDFs $F_1,\ldots,F_n$ respectively and arrival times $T_1,\ldots,T_n$, independent of one another and independent of the $X_i$'s. For simplicity, assume none of the $F_i$'s have point masses and are all invertible (these assumption can be removed by standard techniques). Let $X_{\max}=\max_iX_i$, and let $F_{\max}=\prod_iF_i$ be its CDF. The algorithm fixes a differentiable decreasing function $\alpha:[0,1]\longrightarrow[0,1]$, independent of the $F_i$'s. At any point of time $t\in[0,1]$, if the algorithm sees a value $x$, then it accepts $x$ if $x$ is not in the bottom $\alpha(t)$-quantile of $X_{\max}$ (unless the algorithm has accepted a value already). Equivalently, this condition can be stated as $F_{\max}(x)\geq\alpha(t)$ and (since $F_{\max}$ is invertible) also as $x\geq F_{\max}^{-1}(\alpha(t))$, thus making $\beta(t)=F_{\max}^{-1}(\alpha(t))$ the acceptance threshold at time $t$. Observe that since $\alpha$ is decreasing and $F^{-1}$ is increasing, $\beta$ is a decreasing function. Let the random variable $T$ denote the stopping time of the algorithm, that is, the time at which the algorithm accepts some value. Let $\theta(t)$ denote $\Pr[T\geq t]$. Let $q_{-i}(t)$ denote $\Pr[T\geq t\mid T_i\geq t]$, and note that this is also equal to $\Pr[T\geq t\mid T_i=t]$. We use $x_i$ and $t_i$ for the realizations of the random variables $X_i$ and $T_i$ respectively.

Let the random variable $A$ denote the algorithm's reward, that is, the value picked by the algorithm. We prove competitiveness via the following notion of \textit{approximate stochastic dominance}. We say that a non-negative random variable $Y$ approximately stochastically dominates a non-negative random variable $Z$ by factor $c$ if for all $x\geq0$, we have $\Pr[Y\geq x]\geq c\cdot\Pr[Z\geq x]$. Observe that by integrating both sides of the inequality over $[0,\infty)$, we get $\mathbb{E}[Y]\geq\mathbb{E}[Z]$. Therefore, in order to prove that the algorithm is $c$-competitive, we prove the stronger condition that $A$ approximately stochastically dominates $X_{\max}$ by factor $c$.


\section{Bounds on the stopping time distribution}

\begin{lemma}\label{lem_theta}
\[\theta(t)=\prod_i\left(1-t+\int_0^tF_i(\beta(u))du\right)\text{.}\]
\end{lemma}

\begin{proof}
If the algorithms stops before time $t$, then for some $i$, $x_i$ gets accepted before time $t$. For this to happen, we need $t_i\leq t$ and $x_i\geq\beta(t_i)$. Conversely, if for some $i$ we have $t_i\leq t$ and $x_i\geq\beta(t_i)$, then the algorithm must accept $x_i$ unless it has accepted some $x_j$ already. In any case, the algorithm must stop before time $t$. Thus, the algorithm doesn't stop before time $t$ if and only if \textbf{for all $i$}, either $i$ arrives after time $t$, or its value is less than the threshold at its arrival time (or both). Note that this is a conjunction of independent events, and hence,
\[\theta(t)=\prod_i\Pr[T_i\geq t\vee X_i<\beta(T_i)]=\prod_i\left(\Pr[T_i\geq t]+\Pr[T_i<t\wedge X_i<\beta(T_i)]\right)\text{.}\]
Finally, observe that $\Pr[T_i\geq t]=1-t$ and $\Pr[T_i<t\wedge X_i<\beta(T_i)]=\int_0^tF_i(\beta(u))du$.
\end{proof}

\begin{lemma}
\[q_{-i}(t)=\prod_{j\neq i}\left(1-t+\int_0^tF_j(\beta(u))du\right)\text{.}\]
\end{lemma}

\begin{proof}
Same as the proof of Lemma~\ref{lem_theta} except that, since it is already assured that $X_i$ will not stop the algorithm, the conjunction is over all $j\neq i$.
\end{proof}

The above two lemmas immediately imply,
\begin{equation}\label{eqn_q_theta}
q_{-i}(t)=\frac{\theta(t)}{1-t+\int_0^tF_i(\beta(u))du}\geq\theta(t)\text{,}
\end{equation}
because the denominator, being the probability of an event, is at most $1$.

\begin{lemma}\label{lem_theta_bounds}
\[\exp\left(\int_0^t\ln\alpha(u)du\right)\leq\theta(t)\leq1-t+\int_0^t\alpha(u)du\text{.}\]
\end{lemma}

\begin{proof}
Define $f_i:[t-1,t]\longrightarrow[0,1]$ as $f_i(u)=1$ if $t-1\leq u<0$ if $0\leq u\leq t$ and $f_i(u)=F_i(\beta(u))$. By Lemma~\ref{lem_theta},
\[\theta(t)=\prod_i\left(\int_{t-1}^t f_i(u)du\right)\text{.}\]
For the lower bound on $\theta(t)$, by AM-GM inequality,
\[\int_{t-1}^tf_i(u)du\geq\exp\left(\int_{t-1}^t\ln f_i(u)du\right)\text{,}\]
and therefore,
\begin{eqnarray*}
\theta(t) & \geq & \prod_i\exp\left(\int_{t-1}^t\ln f_i(u)du\right)=\exp\left(\sum_i\int_{t-1}^t\ln f_i(u)du\right)\\
 & & =\exp\left(\int_{t-1}^t\left(\sum_i\ln f_i(u)\right)du\right)=\exp\left(\int_{t-1}^t\ln\left(\prod_i f_i(u)\right)du\right)\text{.}
\end{eqnarray*}
Now for $0\leq u\leq t$, we have $\prod_i f_i(u)=\prod_i F_i(\beta(u))=F_{\max}(\beta(u))=\alpha(u)$, and for $t-1\leq u<0$, $\prod_i f_i(u)=1$. Substituting, we get,
\[\theta(t)\geq\exp\left(\int_0^t\ln\alpha(u)du\right)\text{.}\]
This proves the first inequality. For the upper bound on $\theta(t)$, recall that $\beta$ is a decreasing function whereas each $F_i$ is non-decreasing. Therefore, each $f_i$ is non-increasing. By the continuous version of Chebychev's sum inequality\footnote{\url{https://en.wikipedia.org/wiki/Chebyshev's_sum_inequality\#Continuous_version}}, we get,
\[\theta(t)=\prod_i\left(\int_{t-1}^t f_i(u)du\right)\leq\int_{t-1}^t\left(\prod_if_i(u)\right)du=\int_{t-1}^01\cdot du+\int_0^t\alpha(u)du=1-t+\int_0^t\alpha(u)du\text{.}\]
This proves the second inequality.
\end{proof}


\section{Analysis of the $0.665$-competitive algorithm}

Our goal here is to establish, for an arbitrary $x\geq0$, that $\Pr[A\geq x]$ is bounded from below by some factor $c$ times $\Pr[X_{\max}\geq x]$, where the factor $c$ is completely determined by $x$ and $\alpha$. We can then choose $\alpha$ such that for all $x$, the factor $c$ is at least $0.665$. Recall that the threshold function $\beta$ is non-increasing. We consider three cases depending on the value of $x$ relative to $\beta(1)$ and $\beta(0)$. %Fix an arbitrary $x\geq 0$, and define $t\in[0,1]$ as follows. If $x\in[\beta(1),\beta(0)]$, let $t=\beta^{-1}(x)$. If $x<\beta(1)$, then $t=1$, and if $X>\beta(0)$, then $t=0$.

\begin{lemma}\label{lem_small}
Suppose $x\in[0,\beta(1))$. Then
\[\Pr[A\geq x]\geq\left(1-\int_0^1\alpha(u)du\right)\cdot\Pr[X_{\max}\geq x]\text{.}\]
\end{lemma}

\begin{proof}
Since $x$ is less than the lowest possible threshold, $A\geq x$ holds unless the algorithm ends up not accepting anything. Therefore,
\[\Pr[A\geq x]\geq\Pr[T<1]=1-\theta(1)\geq1-\int_0^1\alpha(u)du\geq\left(1-\int_0^1\alpha(u)du\right)\cdot\Pr[X_{\max}\geq x]\text{,}\]
where the first inequality follows from the upper bound on $\theta(1)$ given by Lemma~\ref{lem_theta_bounds}.
\end{proof}

\begin{lemma}\label{lem_main}
Suppose $x\in[\beta(1),\beta(0)]$. Let $t=\beta^{-1}(x)$. Then
\[\Pr[A\geq x]\geq\left\{\frac{t-\int_0^t\alpha(u)du}{1-\alpha(t)}+\left(\int_t^1\exp\left(\int_0^v\ln\alpha(u)du\right)dv\right)\right\}\cdot\Pr[X_{\max}\geq x]\text{.}\]
\end{lemma}

\begin{proof}
Observe that $A$ is at least $x$ if (and only if) one of the $n+1$ pairwise disjoint events $E_0,E_1\ldots,E_n$ happens, where
\begin{enumerate}
\item[$E_0$:] $T<t$. (Therefore, the threshold at the stopping time is at least $x$. Since the algorithm stopped, it must have accepted a value that is no less than the threshold.)
\item[$E_i$:] $T_i\geq t$, $X_i\geq x$, and the algorithm does not stop before time $T_i$. (Since $x$ itself is more than the threshold at time $T_i$, the algorithm must accept $X_i$.)
\end{enumerate}
For the event $E_0$, by Lemma~\ref{lem_theta_bounds}, we have,
\[\Pr[E_0]=\Pr[T<t]=1-\theta(t)\geq t-\int_0^t\alpha(u)du=\frac{t-\int_0^t\alpha(u)du}{1-F_{\max}(x)}\cdot\Pr[X_{\max}\geq x]=\frac{t-\int_0^t\alpha(u)du}{1-\alpha(t)}\cdot\Pr[X_{\max}\geq x]\text{.}\]
The probability of event $E_i$ is bounded as follows. For $v\in[t,1]$, we have
\[\Pr[E_i\mid T_i=v]=\Pr[T\geq v\wedge X_i\geq x\mid T_i=v]=\Pr[T\geq v\mid X_i\geq x\wedge T_i=v]\cdot\Pr[X_i\geq x\mid T_i=v]\text{.}\]
Observe that whether $T\geq v$ is completely determined by the values of the random variables whose arrival time is less than $v$. Therefore, conditioned on $T_i=v$, the event $T\geq v$ is independent of the value of $X_i$. Also, $X_i$ and $T_i$ are independent random variables. Thus,
\[\Pr[E_i\mid T_i=v]=\Pr[T\geq v\mid T_i=v]\cdot\Pr[X_i\geq x]=q_{-i}(v)\cdot\Pr[X_i\geq x]\geq\theta(v)\cdot\Pr[X_i\geq x]\text{,}\]
where the last inequality follows from Equation~(\ref{eqn_q_theta}). Now we have
\[\Pr[E_i]=\int_t^1\Pr[E_i\mid T_i=v]dv\geq\left(\int_t^1\theta(v)dv\right)\cdot\Pr[X_i\geq x]\text{.}\]
Summing over $i=1,\ldots,n$, we get,
\[\sum_{i=1}^n\Pr[E_i]\geq\left(\int_t^1\theta(v)dv\right)\cdot\left(\sum_i\Pr[X_i\geq x]\right)\geq\left(\int_t^1\exp\left(\int_0^v\ln\alpha(u)du\right)dv\right)\cdot\Pr[X_{\max}\geq x]\text{,}\]
where the last inequality is obtained by using the lower bound on $\theta(v)$ given by Lemma~\ref{lem_theta_bounds}, and the union bound. Adding the probability of $E_0$, we get,
\[\Pr[A\geq x]=\sum_{i=0}^n\Pr[E_i]\geq\left\{\frac{t-\int_0^t\alpha(u)du}{1-\alpha(t)}+\left(\int_t^1\exp\left(\int_0^v\ln\alpha(u)du\right)dv\right)\right\}\cdot\Pr[X_{\max}\geq x]\text{,}\]
as required.
\end{proof}

\begin{lemma}\label{lem_large}
Suppose $x\in(\beta(0),\infty)$. Then
\[\Pr[A\geq x]\geq\left(\int_0^1\exp\left(\int_0^v\ln\alpha(u)du\right)dv\right)\cdot\Pr[X_{\max}\geq x]\text{.}\]
\end{lemma}

\begin{proof}
Same as the proof of Lemma~\ref{lem_main}, taking $t=1$. (Event $E_0$ is empty).
\end{proof}

\begin{theorem}
The algorithm's reward $A$ approximately stochastically dominates $X_{\max}=\max_i X_i$ by factor
\[c(\alpha)=\min\left(\left\{1-\int_0^1\alpha(u)du\right\},\inf_{t\in[0,1]}\left\{\frac{t-\int_0^t\alpha(u)du}{1-\alpha(t)}+\left(\int_t^1\exp\left(\int_0^v\ln\alpha(u)du\right)dv\right)\right\}\right)\]
\end{theorem}

\begin{proof}
Follows easily from Lemmas~\ref{lem_small},\ref{lem_main},\ref{lem_large}.
\end{proof}

\begin{corollary}
The algorithm is $c(\alpha)$-competitive.
\end{corollary}

We are left to choose $\alpha$. Ideally, the function alpha is to be chosen such that $\alpha(1)=0$, so that the first of the two terms in the expression for $c(\alpha)$ is subsumed in the second term, and such that the second term is a constant. This gives a differential equation which is numerically solved to get an approximate solution $\alpha$, and it turns out that $c(\alpha)\geq0.665$.


\section{Analysis of the $0.669$-competitive algorithm}

	Given that $F_{1}$,$F_{2}$,...$F_{n}$ are the initial distribution and $V_{1}$,$V_{2}$,...$V_{n}$ are the corresponding sampled values. For any $t$ $\in [0,1] $, define:\\
	$\theta(t)$ = The probability that nothing is selected till time $t$.\\
	$q_{-i}(t)$ = The probability that nothing is selected till time $t$ conditioned that $V_{i}$ arrives at time $t$.
	$\alpha$ : $[0,1]$ $\rightarrow$ $[0,1]$ and $\beta$ : [0,1] $\rightarrow$ $\mathbb{R}$, both non-increasing such that,
	\begin{align*}
	\Pr[\max_{i} \{V_{1},V_{2},...V_{n}\} \leq \alpha(t)] = \beta(t)
	\end{align*}
	\begin{lemma} $\forall i \in [n], t \in [0,1]$
		\begin{align*}
		q_{-i}(t) = \frac{\theta(t)}{1-t+\int_{0}^{t} \Pr[V_{i} \leq \beta(x)]  dx}
		\end{align*}
	\end{lemma}
	\begin{proof}
		\begin{align*}
		\theta(t) &= \int_{0}^{1} \Pr[T > t \mid t_{i} = x] dx\\
		&= \int_{0}^{t} \Pr[T > t \mid t_{i} = x] dx + \int_{t}^{1} \Pr[T > t \mid t_{i} = x] dx
		\end{align*}
		For any $x$ greater than $t$,
		\begin{align*}
		\Pr[T > t \mid t_{i} = x] = \Pr[T > t \mid t_{i} > t] = q_{-i}(t)
		\end{align*}
		For any $x$ less than $t$,
		\begin{eqnarray*}
			\Pr[T > t \mid t_{i} = x] & = & \Pr[V_{i} \leq \beta(x)] \times \Pr[\text{nothing chosen before time }t \mid V_{i} \leq \beta(x) \wedge t_{i} = x]\\
			& = & \Pr[V_{i} \leq \beta(x)] \times \Pr[\text{nothing chosen before time }t \mid V_{i} \leq \beta(t_i) \wedge t_{i} = x]\\
			& = & \Pr[V_{i} \leq \beta(x)] \times \Pr[\text{nothing chosen before time }t \mid V_{i} \leq \beta(t_i)]\\
			& = & \Pr[V_{i} \leq \beta(x)] \times q_{-i}(t)
		\end{eqnarray*}
		Basically if $V_{i} \leq \beta(t_i)$ then we are sure $i$ is not going to stop the algorithm. Then the stopping time is independent of its arrival time $t_i$.
		
		Putting the inequalities and rearranging gives the result.
	\end{proof}
	\begin{lemma}
		$\forall t \in [0,1]$ and $y \in [0,1/2]$
		\begin{align*}
		\sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-\int_{0}^{y} \Pr[V_{i}>\beta(x)] dx} \geq \frac{\Pr[\max_{i} \{V_{i}\} > \beta(t)]}{1-y\Pr[\max_{i} \{V_{i}\} > \beta(0)]}
		\end{align*}
	\end{lemma}
	\begin{proof}
		\begin{align*}
		\sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-\int_{0}^{y} \Pr[V_{i}>\beta(x)] dx} &= \sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-y+\int_{0}^{y} \Pr[V_{i}\leq\beta(x)] dx}\\
		&\geq \sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-y+y\Pr[V_{i}\leq\beta(0)]}\\
		&=\sum_{i=1}^{n} \frac{1-F_{i}(\beta(t))}{1-y+yF_{i}(\beta(0))}
		\end{align*}
		Using the following inequality	 (Proof in the paper)
		\begin{align*} 
		\frac{1-F_{1}(\beta(t))}{1-y+yF_{1}(\beta(0))} + \frac{1-F_{2}(\beta(t))}{1-y+yF_{2}(\beta(0))} \geq \frac{1-F_{1}(\beta(t))F_{2}(\beta(t))}{1-y+yF_{1}(\beta(0))F_{2}(\beta(0))} 
		\end{align*}  
		and repeating it n times, we get the required result.
	\end{proof}
	For the competitive ratio of 0.669, we define a non-decreasing function $g$, for $t \in\ [0,1]$,
	$$g_{p}(t) = \begin{cases}
	\frac{1}{1-t(1-p)}  \quad \,; t \leq 1/2 \\
	\frac{2}{1+p} \quad \quad \quad; t > 1/2
	\end{cases}
	$$
	Using the result from the last section, if $z$ is the reward, for any $t \in [0,1]$, we can write,\\
	\begin{align*}
	\Pr[z > \beta(t)] &= \frac{1-\theta(t)}{1-\alpha(t)} (1-\alpha(t)) + \sum_{i \in [n]} \Pr[V_{i} > \beta(t)] \int_{t}^{1} q_{-1}(x) dx
	\end{align*}
	Using the bound for $\theta(t)$ derived in the last part, the first term can be written as,
	\begin{align*}
	\frac{1-\theta(t)}{1-\alpha(t)} (1-\alpha(t)) &\geq \frac{\int_{0}^{t} 1 - \alpha(x) dx}{1-\alpha(t)} (\Pr[\max_{i} \{V_{i}\} > \beta_{t}]
	\end{align*}
	For the second term, first we use Lemma 0.1 to write,
	\begin{align*}
	q_{-i}(x) = \frac{\theta(x)}{1-\int_{0}^{x} \Pr[V_{i} > \beta(y)]  dy}
	\end{align*}
	Then interchanging the order of sums,
	\begin{align*}
	\sum_{i \in [n]} \Pr[V_{i} > \beta(t)] \int_{t}^{1} q_{-1}(x) dx = \int_{t}^{1} \theta(x) \sum_{i \in [n]} \frac{\Pr[V_{i} > \beta(t)]}{1 - \int_{0}^{x} \Pr[V_{i} > \beta(y)] dy} dx
	\end{align*}
	Now, using Lemma 0.2, for $x$ $\leq$ $1/2$
	\begin{align*}
	\sum_{i \in [n]} \frac{\Pr[V_{i} > \beta(t)]}{1 - \int_{0}^{x} \Pr[V_{i} > \beta(y)] dy} &\geq \frac{\Pr[\max_{i} \{V_{i}\} > \beta(t)]}{1-x\Pr[\max_{i} \{V_{i}\} > \beta(0)]}\\
	&= g_{\alpha(0)} (x) \Pr[\max_{i} \{V_{i}\} > \beta(t)]
	\end{align*}
	Now, for $x$ $>$ $1/2$, it can be observed that \\
	\begin{align*}
	\sum_{i \in [n]} \frac{\Pr[V_{i} > \beta(t)]}{1 - \int_{0}^{x} \Pr[V_{i} > \beta(y)] dy} &\geq \frac{\Pr[\max_{i} \{V_{i}\} > \beta(t)]}{1-\frac{1}{2}\Pr[\max_{i} \{V_{i}\} > \beta(0)]} \\
	&= g_{\alpha(0)} (x) \Pr[\max_{i} \{V_{i}\} > \beta(t)]
	\end{align*}
	Now, using the bound for $\theta(x)$ derived in the last section,
	\begin{align*}
	\theta(x) \geq e^{\int_{0}^{x} \ln(\alpha(y)) dy}
	\end{align*}
	We get, for each $t \in [0,1]$:
	\begin{align*}
	\Pr[z > \beta(t)] \geq \min_{t}\left(\frac{\int_{0}^{t} 1 - \alpha(x) dx}{1-\alpha(t)} + \int_{t}^{1} e^{\int_{0}^{x} \ln(\alpha(y)) dy} \times g_{\alpha(0)}(x) dx \right) \Pr[\max_{i} \{V_{i}\} > \beta(t)]
	\end{align*}
	We know that the expectation of reward z,
	\begin{align*}
	E[z] = \int_{0}^{\infty} \Pr[z > p] dp
	\end{align*}
	We already know this expression for $p \in [\beta(1),\beta(0)]$. Now, for $p \in [0,\beta(1)]$, first observe that $\Pr[\max_{i} \{V_{i}\} > p]\leq 1$. Then,
	\begin{align*}
	\Pr[z > p] &= 1 - \theta(1)\\
	&\geq \int_{0}^{1} 1 - \alpha(x) dx \\
	&\geq\left(\int_{0}^{1} 1 - \alpha(x) dx \right) \Pr[\max_{i} \{V_{i}\} > p]
	\end{align*}
	And for $p \in\left [\beta(0),\infty\right)$, we can write
	\begin{align*}
	\Pr[z > p] &= \sum_{i=1}^{n} \Pr[V_{i} > p] \int_{0}^{1} q_{-i} (x) dx\\
	&\geq \left(\int_{0}^{1} e^{\int_{0}^{x} \ln(\alpha(y)) dy} \times g_{\alpha(0)}(x) dx \right) \Pr[\max_{i} \{V_{i}\} > p]
	\end{align*}
	The last inequality follows using Lemma 0.1 and 0.2 as done for $p \in [\beta(1),\beta(0)]$\\
	Now, the competitive ratio $\lambda$ can just be written as:
	\begin{align*}
	\lambda = \min \left(\min_{t}\left(\frac{\int_{0}^{t} 1 - \alpha(x) dx}{1-\alpha(t)} + \int_{t}^{1} e^{\int_{0}^{x} \ln(\alpha(y)) dy} \times g_{\alpha(0)}(x) dx \right), \int_{0}^{1} 1 - \alpha(x) dx \right)
	\end{align*}
	Optimization of values for $\alpha$ lead to a competitive ratio of 0.669\\
	\begin{theorem}
	$$\frac{1-F_{1}(\beta(t))}{1-\lambda+\lambda F_{1}(\beta(0))} + \frac{1-F_{2}(\beta(t))}{1-\lambda+\lambda F_{2}(\beta(0))} \geq \frac{1-F_{1}(\beta(t))F_{2}(\beta(t))}{1-\lambda+\lambda F_{1}(\beta(0))F_{2}(\beta(0))}$$
	\end{theorem}
\begin{proof}
	Change the variables as $a = F_{1}(\beta(0))$, b = $F_{2}(\beta(0))$, $x = F_{1}(\beta(t))$ and $y = F_{2}(\beta(t))$ and define a function $h(x,y)$ such that,
	\begin{align*}
	h(x,y) = \frac{1-x}{1-\lambda+\lambda a} + \frac{1-y}{1-\lambda+\lambda b} - \frac{1-xy}{1-\lambda+\lambda ab}
	\end{align*}
	If we can prove that the minima of the function in the interval $x \in [0,a], y\in [0,b]$ is non-negative, then we are done. Note that,
	\begin{align*}
	h_{xx} = 0, h_{yy} = 0, h_{xy} > 0, h_{yx} > 0
	\end{align*}
	which implies that any critical point inside this region is a saddle point, meaning that the minima must lie on the interval boundary. Consider the line $x=0$,
	\begin{align*}
	h(y) = \frac{1}{1-\lambda+\lambda a} + \frac{1-y}{1-\lambda+\lambda b} - \frac{1}{1-\lambda+\lambda ab} \Rightarrow h'(y) < 0
	\end{align*}
	That means the minima of this line lies on the point $(0,b)$. Similarily, for the line $y = 0$, the minima lies on the point $(a,0)$. Now, for the line $x = a$,
	\begin{align*}
	h(y) = \frac{1-a}{1-\lambda+\lambda a} + \frac{1-y}{1-\lambda+\lambda b} - \frac{1-ay}{1-\lambda+\lambda ab} \Rightarrow h'(y) = \frac{(1 - \lambda)(a-1)}{(1-\lambda+\lambda b)(1-\lambda+\lambda ab)} \leq 0
	\end{align*}
	Which means that the minima of this line lies on the point $(a,b)$. Similarily, the minima of the line $y=b$ lies on the point $(a,b)$. Therefore, the local minima of the function is at the point $(a,b)$.
	\begin{align*}
	h(a,b) &= \frac{1-a}{1-\lambda+\lambda a} + \frac{1-b}{1-\lambda+\lambda b} - \frac{1-ab}{1-\lambda+\lambda ab} \\
	&= \frac{(1-a)(1-b)[(1-ab)\lambda^2 - 2\lambda + 1]}{(1-\lambda+\lambda a)(1-\lambda+\lambda b)(1-\lambda+\lambda ab)}\\
	&\geq 0  \quad (\text{For $\lambda \leq 1/2$})
	\end{align*}
\end{proof}

\bibliographystyle{alpha}
\bibliography{references.bib}
\end{document}
