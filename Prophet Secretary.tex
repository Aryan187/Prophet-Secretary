\documentclass[10pt, letterpaper, twoside]{article}
\usepackage[left=2cm, right=2cm, top=1cm]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{xcolor}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
%opening
\title{Prophet Secretary}

\begin{document}
	
	\maketitle
	Given that $F_{1}$,$F_{2}$,...$F_{n}$ are the initial distribution and $V_{1}$,$V_{2}$,...$V_{n}$ are the corresponding sampled values. For any $t$ $\in [0,1] $, define:\\
	$\theta(t)$ = The probability that nothing is selected till time $t$.\\
	$q_{-i}(t)$ = The probability that nothing is selected till time $t$ conditioned that $V_{i}$ arrives at time $t$.
	$\alpha$ : $[0,1]$ $\rightarrow$ $[0,1]$ and $\beta$ : [0,1] $\rightarrow$ $\mathbb{R}$, both non-increasing such that,
	\begin{align*}
	\Pr[\max_{i} \{V_{1},V_{2},...V_{n}\} \leq \alpha(t)] = \beta(t)
	\end{align*}
	\begin{lemma} $\forall i \in [n], t \in [0,1]$
		\begin{align*}
		q_{-i}(t) = \frac{\theta(t)}{1-t+\int_{0}^{t} \Pr[V_{i} \leq \beta(x)]  dx}
		\end{align*}
	\end{lemma}
	\begin{proof}
		\begin{align*}
		\theta(t) &= \int_{0}^{1} \Pr[T > t \mid t_{i} = x] dx\\
		&= \int_{0}^{t} \Pr[T > t \mid t_{i} = x] dx + \int_{t}^{1} \Pr[T > t \mid t_{i} = x] dx
		\end{align*}
		For any $x$ greater than $t$,
		\begin{align*}
		\Pr[T > t \mid t_{i} = x] = \Pr[T > t \mid t_{i} > t] = q_{-i}(t)
		\end{align*}
		For any $x$ less than $t$,
		\begin{eqnarray*}
			\Pr[T > t \mid t_{i} = x] & = & \Pr[V_{i} \leq \beta(x)] \times \Pr[\text{nothing chosen before time }t \mid V_{i} \leq \beta(x) \wedge t_{i} = x]\\
			& = & \Pr[V_{i} \leq \beta(x)] \times \Pr[\text{nothing chosen before time }t \mid V_{i} \leq \beta(t_i) \wedge t_{i} = x]\\
			& = & \Pr[V_{i} \leq \beta(x)] \times \Pr[\text{nothing chosen before time }t \mid V_{i} \leq \beta(t_i)]\\
			& = & \Pr[V_{i} \leq \beta(x)] \times q_{-i}(t)
		\end{eqnarray*}
		Basically if $V_{i} \leq \beta(t_i)$ then we are sure $i$ is not going to stop the algorithm. Then the stopping time is independent of its arrival time $t_i$.
		
		Putting the inequalities and rearranging gives the result.
	\end{proof}
	\begin{lemma}
		$\forall t \in [0,1]$ and $y \in [0,1/2]$
		\begin{align*}
		\sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-\int_{0}^{y} \Pr[V_{i}>\beta(x)] dx} \geq \frac{\Pr[\max_{i} \{V_{i}\} > \beta(t)]}{1-y\Pr[\max_{i} \{V_{i}\} > \beta(0)]}
		\end{align*}
	\end{lemma}
	\begin{proof}
		\begin{align*}
		\sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-\int_{0}^{y} \Pr[V_{i}>\beta(x)] dx} &= \sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-y+\int_{0}^{y} \Pr[V_{i}\leq\beta(x)] dx}\\
		&\geq \sum_{i=1}^{n} \frac{\Pr[V_{i}>\beta(t)]}{1-y+y\Pr[V_{i}\leq\beta(0)]}\\
		&=\sum_{i=1}^{n} \frac{1-F_{i}(\beta(t))}{1-y+yF_{i}(\beta(0))}
		\end{align*}
		Using the following inequality	 (Proof in the paper)
		\begin{align*} 
		\frac{1-F_{1}(\beta(t))}{1-y+yF_{1}(\beta(0))} + \frac{1-F_{2}(\beta(t))}{1-y+yF_{2}(\beta(0))} \geq \frac{1-F_{1}(\beta(t))F_{2}(\beta(t))}{1-y+yF_{1}(\beta(0))F_{2}(\beta(0))} 
		\end{align*}  
		and repeating it n times, we get the required result.
	\end{proof}
	For the competitive ratio of 0.669, we define a non-decreasing function $g$, for $t \in\ [0,1]$,
	$$g_{p}(t) = \begin{cases}
	\frac{1}{1-t(1-p)}  \quad \,; t \leq 1/2 \\
	\frac{2}{1+p} \quad \quad \quad; t > 1/2
	\end{cases}
	$$
	Using the result from the last section, if $z$ is the reward, for any $t \in [0,1]$, we can write,\\
	\begin{align*}
	\Pr[z > \beta(t)] &= \frac{1-\theta(t)}{1-\alpha(t)} (1-\alpha(t)) + \sum_{i \in [n]} \Pr[V_{i} > \beta(t)] \int_{t}^{1} q_{-1}(x) dx
	\end{align*}
	Using the bound for $\theta(t)$ derived in the last part, the first term can be written as,
	\begin{align*}
	\frac{1-\theta(t)}{1-\alpha(t)} (1-\alpha(t)) &\geq \frac{\int_{0}^{t} 1 - \alpha(x) dx}{1-\alpha(t)} (\Pr[\max_{i} \{V_{i}\} > \beta_{t}]
	\end{align*}
	For the second term, first we use Lemma 0.1 to write,
	\begin{align*}
	q_{-i}(x) = \frac{\theta(x)}{1-\int_{0}^{x} \Pr[V_{i} > \beta(y)]  dy}
	\end{align*}
	Then interchanging the order of sums,
	\begin{align*}
	\sum_{i \in [n]} \Pr[V_{i} > \beta(t)] \int_{t}^{1} q_{-1}(x) dx = \int_{t}^{1} \theta(x) \sum_{i \in [n]} \frac{\Pr[V_{i} > \beta(t)]}{1 - \int_{0}^{x} \Pr[V_{i} > \beta(y)] dy} dx
	\end{align*}
	Now, using Lemma 0.2, for $x$ $\leq$ $1/2$
	\begin{align*}
	\sum_{i \in [n]} \frac{\Pr[V_{i} > \beta(t)]}{1 - \int_{0}^{x} \Pr[V_{i} > \beta(y)] dy} &\geq \frac{\Pr[\max_{i} \{V_{i}\} > \beta(t)]}{1-x\Pr[\max_{i} \{V_{i}\} > \beta(0)]}\\
	&= g_{\alpha(0)} (x) \Pr[\max_{i} \{V_{i}\} > \beta(t)]
	\end{align*}
	Now, for $x$ $>$ $1/2$, it can be observed that \\
	\begin{align*}
	\sum_{i \in [n]} \frac{\Pr[V_{i} > \beta(t)]}{1 - \int_{0}^{x} \Pr[V_{i} > \beta(y)] dy} &\geq \frac{\Pr[\max_{i} \{V_{i}\} > \beta(t)]}{1-\frac{1}{2}\Pr[\max_{i} \{V_{i}\} > \beta(0)]} \\
	&= g_{\alpha(0)} (x) \Pr[\max_{i} \{V_{i}\} > \beta(t)]
	\end{align*}
	Now, using the bound for $\theta(x)$ derived in the last section,
	\begin{align*}
	\theta(x) \geq e^{\int_{0}^{x} \ln(\alpha(y)) dy}
	\end{align*}
	We get, for each $t \in [0,1]$:
	\begin{align*}
	\Pr[z > \beta(t)] \geq \min_{t}\left(\frac{\int_{0}^{t} 1 - \alpha(x) dx}{1-\alpha(t)} + \int_{t}^{1} e^{\int_{0}^{x} \ln(\alpha(y)) dy} \times g_{\alpha(0)}(x) dx \right) \Pr[\max_{i} \{V_{i}\} > \beta(t)]
	\end{align*}
	We know that the expectation of reward z,
	\begin{align*}
	E[z] = \int_{0}^{\infty} \Pr[z > p] dp
	\end{align*}
	We already know this expression for $p \in [\beta(1),\beta(0)]$. Now, for $p \in [0,\beta(1)]$, first observe that $\Pr[\max_{i} \{V_{i}\} > p]\leq 1$. Then,
	\begin{align*}
	\Pr[z > p] &= 1 - \theta(1)\\
	&\geq \int_{0}^{1} 1 - \alpha(x) dx \\
	&\geq\left(\int_{0}^{1} 1 - \alpha(x) dx \right) \Pr[\max_{i} \{V_{i}\} > p]
	\end{align*}
	And for $p \in\left [\beta(0),\infty\right)$, we can write
	\begin{align*}
	\Pr[z > p] &= \sum_{i=1}^{n} \Pr[V_{i} > p] \int_{0}^{1} q_{-i} (x) dx\\
	&\geq \left(\int_{0}^{1} e^{\int_{0}^{x} \ln(\alpha(y)) dy} \times g_{\alpha(0)}(x) dx \right) \Pr[\max_{i} \{V_{i}\} > p]
	\end{align*}
	The last inequality follows using Lemma 0.1 and 0.2 as done for $p \in [\beta(1),\beta(0)]$\\
	Now, the competitive ratio $\lambda$ can just be written as:
	\begin{align*}
	\lambda = \min \left(\min_{t}\left(\frac{\int_{0}^{t} 1 - \alpha(x) dx}{1-\alpha(t)} + \int_{t}^{1} e^{\int_{0}^{x} \ln(\alpha(y)) dy} \times g_{\alpha(0)}(x) dx \right), \int_{0}^{1} 1 - \alpha(x) dx \right)
	\end{align*}
	Optimization of values for $\alpha$ lead to a competitive ratio of 0.669\\
	\begin{theorem}
	$$\frac{1-F_{1}(\beta(t))}{1-\lambda+\lambda F_{1}(\beta(0))} + \frac{1-F_{2}(\beta(t))}{1-\lambda+\lambda F_{2}(\beta(0))} \geq \frac{1-F_{1}(\beta(t))F_{2}(\beta(t))}{1-\lambda+\lambda F_{1}(\beta(0))F_{2}(\beta(0))}$$
	\end{theorem}
\begin{proof}
	Change the variables as $a = F_{1}(\beta(0))$, b = $F_{2}(\beta(0))$, $x = F_{1}(\beta(t))$ and $y = F_{2}(\beta(t))$ and define a function $h(x,y)$ such that,
	\begin{align*}
	h(x,y) = \frac{1-x}{1-\lambda+\lambda a} + \frac{1-y}{1-\lambda+\lambda b} - \frac{1-xy}{1-\lambda+\lambda ab}
	\end{align*}
	If we can prove that the minima of the function in the interval $x \in [0,a], y\in [0,b]$ is non-negative, then we are done. Note that,
	\begin{align*}
	h_{xx} = 0, h_{yy} = 0, h_{xy} > 0, h_{yx} > 0
	\end{align*}
	which implies that any critical point inside this region is a saddle point, meaning that the minima must lie on the interval boundary. Consider the line $x=0$,
	\begin{align*}
	h(y) = \frac{1}{1-\lambda+\lambda a} + \frac{1-y}{1-\lambda+\lambda b} - \frac{1}{1-\lambda+\lambda ab} \Rightarrow h'(y) < 0
	\end{align*}
	That means the minima of this line lies on the point $(0,b)$. Similarily, for the line $y = 0$, the minima lies on the point $(a,0)$. Now, for the line $x = a$,
	\begin{align*}
	h(y) = \frac{1-a}{1-\lambda+\lambda a} + \frac{1-y}{1-\lambda+\lambda b} - \frac{1-ay}{1-\lambda+\lambda ab} \Rightarrow h'(y) = \frac{(1 - \lambda)(a-1)}{(1-\lambda+\lambda b)(1-\lambda+\lambda ab)} \leq 0
	\end{align*}
	Which means that the minima of this line lies on the point $(a,b)$. Similarily, the minima of the line $y=b$ lies on the point $(a,b)$. Therefore, the local minima of the function is at the point $(a,b)$.
	\begin{align*}
	h(a,b) &= \frac{1-a}{1-\lambda+\lambda a} + \frac{1-b}{1-\lambda+\lambda b} - \frac{1-ab}{1-\lambda+\lambda ab} \\
	&= \frac{(1-a)(1-b)[(1-ab)\lambda^2 - 2\lambda + 1]}{(1-\lambda+\lambda a)(1-\lambda+\lambda b)(1-\lambda+\lambda ab)}\\
	&\geq 0  \quad (\text{For $\lambda \leq 1/2$})
	\end{align*}
\end{proof}
\end{document}
